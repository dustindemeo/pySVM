#NAME: Variable.py
#DATE CREATED: 12/1/2014
#AUTHORS: Abhinandan Halemane, Dustin DeMeo, Mahshid Aimaq, Vinitha Raja

'''
DESCRIPTION:
This module has the code create a SVM by passing in necessary data and parameters. 

Future plans include turning it into a class with attributes to hold results and methods to return the results (ex. for graphing purposes). Also, turning the code into a class will allow it to be modularized into relevant functions (ex. gridding, cross-validating, training, testing, etc).

The only function is the skSVM which does all SVM functionality and prints out results to the terminal.

MODIFICATION HISTORY:
12/1/14: File created. Using hard coded data and Dr. Wesley's code.
Author: Abhinandan Halemane, Dustin DeMeo, Mahshid Aimaq, Vinitha Raja
Description: copied Dr. Wesleys code to understand how the basic functionality works
12/10/14: Rewrote code to use Variable class and basic GUI
Author: Abhinandan Halemane, Dustin DeMeo, Mahshid Aimaq, Vinitha Raja
Description: Created a variable class to hold and feed in variables. Moved Variable class to its own module. Fed in GUI parameters to make X and y. Separately passed in split size.
12/12/14: Wrote GUI for parameters and integrated
Author: Abhinandan Halemane, Dustin DeMeo, Mahshid Aimaq, Vinitha Raja
Description: Passing in tuned_parameters from a GUI window. Rewrote code to save different clfs based on different scoring into an array
12/16/14: Wrote improved train/test splitting functionality (with stratify), multiple cross validation options, and improved reporting
Author: Abhinandan Halemane, Dustin DeMeo, Mahshid Aimaq, Vinitha Raja
Description: Train/test now split with cross validation tools to allow for stratification. Cross validation options now only operate on training data, but also give all scoring metrics. Also, cross validation functionality includes different data splitting options.
'''

import pprint
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.cross_validation import cross_val_score, KFold, StratifiedKFold, LeaveOneOut, LeavePOut, ShuffleSplit, StratifiedShuffleSplit

def skSVM(X, y, scoring, tuned_parameters, data_parameters, cv_parameters):
    '''DESCRIPTION: This function takes the data set, already processed, along with relevant parameters. It then generates models, selects the best model with the training set, trains the best model with the training dataset, and tests the model on the test dataset. This includes spliting the data into training and testing, gridding, generating cross-validation metrics for all score types, training, testing, and reporting to the terminal.
    PRECONDITIONS: X. This is the independent variable dataset. It must be a numpy float array that is already preprocessed (per the function process in the guiTK module). No empty vaues allowed.
                   y. This is the dependent variable dataset. It must be a numpy float array that is already preprocessed and has the same number of elements as rows in the X set. No empty values allowed. Binary categories (0 and 1) only with at least one of each.
                   scoring. One of the standard scikit learn scoring values (ex. accuracy, precision ...)
                   tuned parameters. The standard scikit learn tuned_parameters list of dictionaries (ex. kernel, ...).
                   data_parameters. This is a dictionary of the parameters used for the data. The relevant ones here are how to split the datafile. Generated by process.
                   cv_parameters. This is a dictionary of all options required to implement the cross validations. Generated by process.
    POSTCONDITIONS: Print to terminal. This function prints out results to the terminal including the best estimator, cross validation results on the training data, and prediction metrics from the testing data
    SIDE EFFECTS: None.
    RETURN: None. dummy function prints. Future work will make it initialize and store variables in a class
    '''


    # Users have the option of stratifying data, or guaranteeing that an equal number of pass/fail are in both testing and training sets. Uses Cross Validation instead of simple test split function
    # First we create the mask for sorting, then we apply it to new numpy arrays
    if data_parameters['stratify']:
        split = StratifiedKFold(y=y, n_folds=int(1./data_parameters['testSize']), shuffle=True, random_state=data_parameters['random'])
    else:
        split = KFold(n=len(y), n_folds=int(1./data_parameters['testSize']), shuffle=True, random_state=data_parameters['random'])

    for train, test in split:
        X_train = X[train]
        X_test = X[test]
        y_train = y[train]
        y_test = y[test]
        break

    print "Starting the gridding process."
    print

    # We must guarantee that we have a valid number of folds, must be at least two but no lower that the mininum count of either DV class or five samples per fold
    numLabel1 = np.count_nonzero(y_train)
    numLabel0 = len(y_train) - numLabel1
    numFiveFolds = len(y_train) % 5
    n_folds = max(2, min(cv_parameters['folds'], numLabel0, numLabel1, numFiveFolds))

    # Build a cross validation data sorting method to pass into the gridding process based on data_parameters (from the GUI input)
    if cv_parameters['cvType'] == 'skf':
        cv = StratifiedKFold(y=y_train, n_folds=n_folds, shuffle=True, random_state=data_parameters['random'])
    elif cv_parameters['cvType'] == 'kf':
        cv = KFold(n=len(y_train), n_folds=n_folds, shuffle=True, random_state=data_parameters['random'])
    elif cv_parameters['cvType'] == 'sss':
        cv = StratifiedShuffleSplit(y=y_train, n_iter=cv_parameters['nIter'], test_size=cv_parameters['testSize'], random_state=data_parameters['random'])
    elif cv_parameters['cvType'] == 'ss':
        cv = ShuffleSplit(n=len(y_train), n_iter=cv_parameters['nIter'], test_size=cv_parameters['testSize'], random_state=data_parameters['random'])
    elif cv_parameters['cvType'] == 'lolo':
        cv = LeaveOneOut(n=len(y_train))
    elif cv_parameters['cvType'] == 'lplo':
        cv = LeavePOut(n=len(y_train), p=cv_parameters['p'])

    # Now grid based on the scoring, tuned parameters and CV class above
    grid = GridSearchCV(estimator=SVC(), param_grid=tuned_parameters, scoring = scoring, cv=cv)
    grid.fit(X_train, y_train)

    # Retrain the best CLF from the gridding process with the entire training data set and cross validate against all scoring types
    clf = grid.best_estimator_
    clf.fit(X_train, y_train)
    accuracy_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=cv)
    precision_scores = cross_val_score(clf, X_train, y_train, scoring='precision', cv=cv)
    recall_scores = cross_val_score(clf, X_train, y_train, scoring='recall', cv=cv)
    f1_scores = cross_val_score(clf, X_train, y_train, scoring='f1', cv=cv)
    y_test_pred = clf.predict(X_test)
    y_train_pred = clf.predict(X_train)

    # Print the relevant results to the terminal for 
    # 1) The best estimator parameters
    # 2) Cross validation results for the training data and best estimator
    # 3) Prediction metrics on the test data
    print
    print "ESTIMATOR SELECTED FOR OPTIMAL {}:".format(grid.get_params(deep=True)['scoring'].upper())
    print "Parameters:"
    pprint.pprint(grid.best_params_, width=1)
    print
    print "Cross-validation scores on training data:"
    print "  Accuracy:  {:.1f} +/- {:.1f}%".format(accuracy_scores.mean() * 100, accuracy_scores.std() * 100)
    print "  Precision: {:.1f} +/- {:.1f}%".format(precision_scores.mean() * 100, precision_scores.std() * 100)
    print "  Recall:    {:.1f} +/- {:.1f}%".format(recall_scores.mean() * 100, recall_scores.std() * 100)
    print "  F1:        {:.1f} +/- {:.1f}%".format(f1_scores.mean() * 100, f1_scores.std() * 100)
    print
    print "Trained estimator scores on testing data:"
    print "  Accuracy:  {:.1f}%".format(accuracy_score(y_test, y_test_pred)*100)
    print "  Precision: {:.1f}%".format(precision_score(y_test, y_test_pred, average='weighted')*100)
    print "  Recall:    {:.1f}%".format(recall_score(y_test, y_test_pred, average='weighted')*100)
    print "  F1:        {:.1f}%".format(f1_score(y_test, y_test_pred, average='weighted')*100)
    print
    print "Trained estimator classification report on testing data:"
    print
    print classification_report(y_test, y_test_pred, target_names=['Fail', 'Pass'])

